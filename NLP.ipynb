{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d264eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:29:55.175484Z",
     "start_time": "2021-11-15T07:29:55.163765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/ali/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ali/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ali/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32ad163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:20:36.790280Z",
     "start_time": "2021-11-15T07:20:36.783547Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e747b",
   "metadata": {},
   "source": [
    "1. Read the data using pandas \n",
    "2. Keep text and sentiment and drop the other columns\n",
    "3. Tokenize\n",
    "4. Remove punctuations\n",
    "5. **Remove numbers**\n",
    "6. Remove words of 1 or 2 characters\n",
    "7. Remove stop words\n",
    "8. Convert to lower case\n",
    "9. Use stemmer\n",
    "10. Create Bag of Words\n",
    "11. Compute TF\n",
    "12. Put a threshold for TF and retain only those words above the threshold TF\n",
    "13. Finally u will get the Term Document Matrix. This will be the independent variable set: They will be the X variables\n",
    "14. Use the sentiment column as the label: y\n",
    "15. X & y will be a supervised classification problem\n",
    "16. Use any classification algorithm like random forest for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b799589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T03:03:13.743761Z",
     "start_time": "2021-11-15T03:03:13.731599Z"
    }
   },
   "source": [
    "## Read the data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae875217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:20:45.498764Z",
     "start_time": "2021-11-15T07:20:43.950724Z"
    }
   },
   "outputs": [],
   "source": [
    "# n_samples = 100\n",
    "n_samples = 2_000\n",
    "# read data\n",
    "data = pd.read_csv('data/imdb_master.csv')\n",
    "# remove unsupervised data\n",
    "data=data[data.label != 'unsup']\n",
    "# take a sample from the data as df\n",
    "df= data.sample(n_samples, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fc997b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T03:06:25.941942Z",
     "start_time": "2021-11-15T03:06:25.935912Z"
    }
   },
   "source": [
    "## Keep text and sentiment and drop the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e742abfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:20:54.027219Z",
     "start_time": "2021-11-15T07:20:53.956644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    1015\n",
       "pos     985\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Unnamed: 0', 'file', 'type'], axis=1)\n",
    "df.columns = ['review', 'sentiment']\n",
    "df=df[df.sentiment != 'unsup']\n",
    "\n",
    "# n_samples = 2_000\n",
    "# n_samples = 100\n",
    "# df = data.sample(n_samples, random_state=0)\n",
    "\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88686601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T03:05:12.028671Z",
     "start_time": "2021-11-15T03:05:12.020467Z"
    }
   },
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0686ba75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:20:54.342116Z",
     "start_time": "2021-11-15T07:20:54.340374Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    #Tokenize words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4810d9",
   "metadata": {},
   "source": [
    "## Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae17d20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:20:54.625014Z",
     "start_time": "2021-11-15T07:20:54.622004Z"
    }
   },
   "outputs": [],
   "source": [
    "def normailze_text(text):\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra characters\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    \n",
    "    # Remove punctuation characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text) \n",
    "    \n",
    "    # Remove symbols\n",
    "    text = re.sub(r'[^A-Za-z\\s]',r'',text)\n",
    "    text = re.sub(r'\\n',r'',text)\n",
    "    # Remove two characters\n",
    "    resulst =[]\n",
    "    for i in text.split(' '):\n",
    "        resulst.append(re.sub(r'^\\w{0,2}$',r'',i))\n",
    "    text = ' '.join(resulst) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a0e44cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:20:54.925657Z",
     "start_time": "2021-11-15T07:20:54.923735Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    token_list = []\n",
    "    \n",
    "    for word in tokens:\n",
    "        if not word in stop_words:\n",
    "            token_list.append(word)\n",
    "            \n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d1cf0",
   "metadata": {},
   "source": [
    "## Use stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070e9b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:20:55.201611Z",
     "start_time": "2021-11-15T07:20:55.199650Z"
    }
   },
   "outputs": [],
   "source": [
    "def stem_lem_words(tokens):\n",
    "    \n",
    "#     # Stemming tokens\n",
    "#     tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "#     print('\\nStemming Output:\\n')\n",
    "#     print(tokens)\n",
    "    \n",
    "    #Lemmatizing tokens\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89463d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:21:00.463109Z",
     "start_time": "2021-11-15T07:20:55.646684Z"
    }
   },
   "outputs": [],
   "source": [
    "df['result'] = df.review.apply(normailze_text)\n",
    "df['result'] = df.result.apply(tokenize_text)\n",
    "df['result'] = df.result.apply(remove_stopwords)\n",
    "df['result'] = df.result.apply(stem_lem_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51a8986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:21:00.870791Z",
     "start_time": "2021-11-15T07:21:00.858045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>Al Pacino was once an actor capable of making ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[pacino, actor, capable, make, role, work, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>If you read the book by Carl Hiaasen, the movi...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[read, book, carl, hiaasen, movie, follow, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>This movie is sort of a Carrie meets Heavy Met...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[movie, sort, carrie, meet, heavy, metal, high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>This movie was like a bad indie with A-list ta...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[movie, like, bad, indie, list, talent, plot, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>In the '70s, Charlton Heston starred in sci-fi...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[charlton, heston, star, sci, flick, vary, qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Just had the misfortune to see this truly awfu...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[misfortune, see, truly, awful, film, think, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27269</th>\n",
       "      <td>To confess having fantasies about Brad Pitt is...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[confess, fantasy, brad, pitt, pretty, tough, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Where do I begin, its one of the most frustrat...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[begin, one, frustrate, movies, see, make, lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>I like bad films, but this thing is a steaming...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[like, bad, film, thing, steam, heap, shaky, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33679</th>\n",
       "      <td>You can tell a Lew Grade production a mile off...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[tell, lew, grade, production, mile, distinctl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "11841  Al Pacino was once an actor capable of making ...       neg   \n",
       "19602  If you read the book by Carl Hiaasen, the movi...       pos   \n",
       "45519  This movie is sort of a Carrie meets Heavy Met...       pos   \n",
       "25747  This movie was like a bad indie with A-list ta...       neg   \n",
       "42642  In the '70s, Charlton Heston starred in sci-fi...       pos   \n",
       "...                                                  ...       ...   \n",
       "6087   Just had the misfortune to see this truly awfu...       neg   \n",
       "27269  To confess having fantasies about Brad Pitt is...       neg   \n",
       "455    Where do I begin, its one of the most frustrat...       neg   \n",
       "11984  I like bad films, but this thing is a steaming...       neg   \n",
       "33679  You can tell a Lew Grade production a mile off...       neg   \n",
       "\n",
       "                                                  result  \n",
       "11841  [pacino, actor, capable, make, role, work, wit...  \n",
       "19602  [read, book, carl, hiaasen, movie, follow, pre...  \n",
       "45519  [movie, sort, carrie, meet, heavy, metal, high...  \n",
       "25747  [movie, like, bad, indie, list, talent, plot, ...  \n",
       "42642  [charlton, heston, star, sci, flick, vary, qua...  \n",
       "...                                                  ...  \n",
       "6087   [misfortune, see, truly, awful, film, think, s...  \n",
       "27269  [confess, fantasy, brad, pitt, pretty, tough, ...  \n",
       "455    [begin, one, frustrate, movies, see, make, lot...  \n",
       "11984  [like, bad, film, thing, steam, heap, shaky, c...  \n",
       "33679  [tell, lew, grade, production, mile, distinctl...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f08063f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:29:33.837737Z",
     "start_time": "2021-11-15T07:29:33.823928Z"
    }
   },
   "outputs": [],
   "source": [
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \" \" \n",
    "    # return string  \n",
    "    return (str1.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de94a8b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:29:58.625824Z",
     "start_time": "2021-11-15T07:29:58.401814Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bowmatrix = vectorizer.fit_transform(df[\"result\"].apply(listToString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7222d905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:29:59.438575Z",
     "start_time": "2021-11-15T07:29:59.276361Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(bowmatrix.toarray(), columns=vectorizer.get_feature_names()) # TERM DOCUEMNT MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73e8a2d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:30:03.465397Z",
     "start_time": "2021-11-15T07:30:03.455101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aadha</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aames</th>\n",
       "      <th>aamir</th>\n",
       "      <th>aap</th>\n",
       "      <th>aapke</th>\n",
       "      <th>aardman</th>\n",
       "      <th>aargh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>...</th>\n",
       "      <th>zonked</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoog</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoot</th>\n",
       "      <th>zora</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 20423 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aadha  aah  aaliyah  aames  aamir  aap  aapke  aardman  aargh  aaron  \\\n",
       "0         0    0        0      0      0    0      0        0      0      0   \n",
       "1         0    0        0      0      0    0      0        0      0      0   \n",
       "2         0    0        0      0      0    0      0        0      0      0   \n",
       "3         0    0        0      0      0    0      0        0      0      0   \n",
       "4         0    0        0      0      0    0      0        0      0      0   \n",
       "...     ...  ...      ...    ...    ...  ...    ...      ...    ...    ...   \n",
       "1995      0    0        0      0      0    0      0        0      0      0   \n",
       "1996      0    0        0      0      0    0      0        0      0      0   \n",
       "1997      0    0        0      0      0    0      0        0      0      0   \n",
       "1998      0    0        0      0      0    0      0        0      0      0   \n",
       "1999      0    0        0      0      0    0      0        0      0      0   \n",
       "\n",
       "      ...  zonked  zoo  zoog  zoolander  zoom  zoot  zora  zucker  zuckerman  \\\n",
       "0     ...       0    0     0          0     0     0     0       0          0   \n",
       "1     ...       0    0     0          0     0     0     0       0          0   \n",
       "2     ...       0    0     0          0     0     0     0       0          0   \n",
       "3     ...       0    0     0          0     0     0     0       0          0   \n",
       "4     ...       0    0     0          0     0     0     0       0          0   \n",
       "...   ...     ...  ...   ...        ...   ...   ...   ...     ...        ...   \n",
       "1995  ...       0    0     0          0     0     0     0       0          0   \n",
       "1996  ...       0    0     0          0     0     0     0       0          0   \n",
       "1997  ...       0    0     0          0     0     0     0       0          0   \n",
       "1998  ...       0    0     0          0     0     0     0       0          0   \n",
       "1999  ...       0    0     0          0     0     0     0       0          0   \n",
       "\n",
       "      zulu  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "...    ...  \n",
       "1995     0  \n",
       "1996     0  \n",
       "1997     0  \n",
       "1998     0  \n",
       "1999     0  \n",
       "\n",
       "[2000 rows x 20423 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ffa2a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:31:14.657604Z",
     "start_time": "2021-11-15T07:30:48.363536Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.382434  ,   9.23344502,  11.04729668, ..., 100.        ,\n",
       "       100.        , 100.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_6 = PCA()\n",
    "pca_6.fit(new_df)\n",
    "np.cumsum(pca_6.explained_variance_ratio_*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0685b5ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T07:30:42.595352Z",
     "start_time": "2021-11-15T07:30:42.590456Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
